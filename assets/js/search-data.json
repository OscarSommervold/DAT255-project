{
  
    
        "post0": {
            "title": "Read data and create DataFrames",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import pickle import skimage.draw as sk from fastai.vision.all import * from fastai.imports import * . train = pd.read_csv(&#39;../input/forestnet/ForestNetDataset/train.csv&#39;) val = pd.read_csv(&#39;../input/forestnet/ForestNetDataset/val.csv&#39;) . train.drop(columns = [&#39;label&#39;,&#39;latitude&#39;,&#39;longitude&#39;,&#39;year&#39;], inplace = True) val.drop(columns = [&#39;label&#39;,&#39;latitude&#39;,&#39;longitude&#39;,&#39;year&#39;], inplace = True) train[&#39;is_valid&#39;] = False val[&#39;is_valid&#39;] = True imgset = pd.concat([train,val],ignore_index = True) imgset.head() . merged_label example_path is_valid . 0 Plantation | examples/4.430849118860583_96.1016343478138 | False | . 1 Other | examples/1.3323406178609702_109.37422873130464 | False | . 2 Grassland shrubland | examples/-1.7202663845775041_115.00699582064483 | False | . 3 Smallholder agriculture | examples/-2.248346072674411_104.1357857482906 | False | . 4 Plantation | examples/-2.100800102991412_113.02237632340159 | False | . create dictionary with our labels and their corresponding pixel value . codes_dict = {&quot;Undefined&quot;: 0, &quot;Plantation&quot;: 1, &quot;Smallholder agriculture&quot;: 2,&quot;Other&quot;: 3,&quot;Grassland shrubland&quot;: 4} codes_dict.values() . dict_values([0, 1, 2, 3, 4]) . Semantic segmentation data prep . each pixel inside the forest loss region is linked to a single class. This means that everything outside the forest loss region is undefined, and the result is a binary masks with labeled pixels inside the forest loss region and undefined pixels outside. In reality several deforestation drivers (labels) can be present in the same image, which is why my approach is to do per pixel segmentation on the entire image. . The Forest loss regions are stored as pickle files and consists of MultiPolygons or single Polygons. As far as I know this datatype cannot be used in deep learning models so they have to be converted to PILMasks. . Iterate the Polygons and get their exterior coordinates | Fill the outlines using the polygon2mask function https://scikit-image.org/docs/dev/api/skimage.draw.html#skimage.draw.polygon2mask | Map the mask(s) onto a zero initialized array with the correct resolution (in this case 332 x 332 pixels) using np.logical_or https://numpy.org/doc/stable/reference/generated/numpy.logical_or.html. This leaves us with a binary mask where ones are the forest loss region and zeroes are undefined. | Fill the values inside the forest loss region with the appropriate pixel value from our codes dictionary (1 for Plantation etc). | def getEdges(polygon): edge = [] for i in polygon.exterior.coords: x,y = i x = np.round(x).astype(int) y = np.round(y).astype(int) edge.append((x,y)) return edge . def createPILMask(poly,label,codes): mask = np.zeros((332,332)) edge = [] if (poly.type == &#39;Polygon&#39;): edge = getEdges(poly) pmask = sk.polygon2mask((332,332),edge) mask = np.logical_or(mask,pmask) else: for polygon in poly: edge = getEdges(polygon) pmask = sk.polygon2mask((332,332),edge) mask = np.logical_or(mask,pmask) mask = np.uint8(mask) mask = mask * labelToInt(label,codes) mask = np.rot90(mask,1) return PILMask.create(mask) . def labelToInt(label,codes): for j,i in enumerate(codes): if (i == label): return j . def getForestLoss(path): with open(&#39;../input/forestnet/ForestNetDataset/&#39;+ path +&#39;/forest_loss_region.pkl&#39;, &#39;rb&#39;) as f: return pickle.load(f) . Find suitable weights for the labels . The labels are unevenly represented in the dataset. An intuitive approach would be to normalize the distribution and pass them as weights to the loss function in order to counter the imbalance of occurences in the dataset. This is the approach I went for although I suspect it is far from optimal. I would have liked to explore this step further as the weights greatly influence the performance of the model. . train_count = train.merged_label.value_counts() train_count . Plantation 686 Smallholder agriculture 556 Other 231 Grassland shrubland 143 Name: merged_label, dtype: int64 . val_count = val.merged_label.value_counts() val_count . Plantation 218 Smallholder agriculture 140 Other 70 Grassland shrubland 45 Name: merged_label, dtype: int64 . norm_train = np.linalg.norm(np.array(train_count.values)) norm_val = np.linalg.norm(np.array(val_count.values)) . n_1 = ((train_count / norm_train).values) n_2 = ((val_count / norm_val).values) weights = (n_1 + n_2) / 2 # Assign a weight of zero to the &quot;Undefined&quot; label. weights = np.insert(weights[::-1],0,0.0) print(codes_dict.keys()) weights . dict_keys([&#39;Undefined&#39;, &#39;Plantation&#39;, &#39;Smallholder agriculture&#39;, &#39;Other&#39;, &#39;Grassland shrubland&#39;]) . array([0. , 0.16007583, 0.25363712, 0.558147 , 0.77182238]) . Put the data in a DataBlock . since I created a is_valid column in my dataframe I can use the built in ColSplitter() to parse the training and validation data. Like in the paper I resize all images to 160x160 and randomly crop during training and center crop during prediction on validation/test. Ideally I would have experimented more with data augmentations but I ran out of GPU Quota. The variations I tried such as Dihedral, flips and zoom yielded no significant performance improvement. . db = DataBlock(blocks = (ImageBlock, MaskBlock(codes = codes_dict)), splitter = ColSplitter(), get_x = lambda x: &quot;../input/forestnet/ForestNetDataset/&quot;f&#39;{x[1]}&#39;&quot;/images/visible/composite.png&quot;, get_y = lambda x: createPILMask(getForestLoss(x[1]),x[0],codes_dict), item_tfms=Resize(160,method = &#39;crop&#39;), batch_tfms=[Normalize.from_stats(*imagenet_stats)]) . dls = db.dataloaders(imgset, bs = 4) dls.show_batch(vmin = 0,vmax = 5) . metrics . from https://github.com/walkwithfastai/walkwithfastai.github.io/blob/master/nbs/course2020/vision/04_Segmentation.ipynb I ignore the &quot;Undefined&quot; label when calculating pixel accuracy. . def accuracy(inp, targ): targ = targ.squeeze(1) # skip &quot;Undefined&quot; mask = targ != 0 return (inp.argmax(dim=1)[mask]==targ[mask]).float().mean() . Learner and training . I initially used standard CrossEntropyLossFlat, but the LabelSmoothing-version yielded noticeable better results so I swapped. I suspect it is because the predicted probability values are pretty low for all labels. . learn = unet_learner(dls, resnet34, metrics = accuracy, loss_func = LabelSmoothingCrossEntropyFlat(axis = 1, weight = tensor(weights).cuda())) learn.loss_func . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . FlattenedLoss of LabelSmoothingCrossEntropy() . learn.fine_tune(8) . epoch train_loss valid_loss accuracy time . 0 | 1.435736 | 1.327350 | 0.585658 | 02:00 | . epoch train_loss valid_loss accuracy time . 0 | 1.117980 | 1.140824 | 0.608729 | 01:57 | . 1 | 1.223691 | 1.191687 | 0.623133 | 01:55 | . 2 | 1.239466 | 1.111991 | 0.636904 | 01:56 | . 3 | 1.165861 | 1.352407 | 0.642428 | 01:56 | . 4 | 1.039873 | 1.110148 | 0.681547 | 01:56 | . 5 | 0.878665 | 1.046393 | 0.681730 | 01:54 | . 6 | 0.743197 | 0.991674 | 0.726310 | 01:56 | . 7 | 0.653169 | 1.004369 | 0.741609 | 01:57 | . Validation accuracy . I do per pixel segmentation on the entire image. However, I am only interested in the forest loss region. I solve this by going through these steps . UNet gives a probability map for each label (in this case output shape is 5x160x160 per prediction), so in order to compose a segmented image I take the maximum probability value for each pixel with argmax (now i have a 160x160 mask). | look at the pixel values inside the forest loss region . | The pixel with the most occurences is the predicted label (so if 60% of the pixels are 2, the predicted label is &quot;Smallholder agriculture&quot; etc). | preds = learn.get_preds() preds[0].shape . (473, 5, 160, 160) . def label_predict(target,pred,codes,test): maskindices = np.nonzero(target) if test: maskindices = zip(maskindices[0],maskindices[1]) pred_labels = pred.argmax(dim = 0) label_vals = [pred_labels[i[0]][i[1]].item() for i in maskindices] return list(codes)[scipy.stats.mode(label_vals)[0][0]] . def accuracy(preds,df,codes): acc = {} # Ideally I dont want any predictions for the undefined label, unfortunately there are a few for most labels undefined_count = 0 for label in df.merged_label.value_counts().index: label_indices = df[df.merged_label == label].index acc[label] = [label_predict(preds[1][ind],preds[0][ind],codes_dict,False) for ind in label_indices] for key in acc.keys(): vals = acc[key] # undefined_count += len([x for x in vals if &quot;Undefined&quot; in x]) acc[key] = len([x for x in vals if key in x]) / len(vals) # print(&#39;undefined&#39;, undefined_count) return acc . vset = imgset[imgset.is_valid == True] vset.reset_index(drop = True, inplace = True) accuracy(preds,vset,codes_dict) . {&#39;Plantation&#39;: 0.8165137614678899, &#39;Smallholder agriculture&#39;: 0.5428571428571428, &#39;Other&#39;: 0.3142857142857143, &#39;Grassland shrubland&#39;: 0.6} . def predict_mask(target,pred,test): maskindices = np.nonzero(target) if test: maskindices = zip(maskindices[0],maskindices[1]) pred_labels = pred.argmax(dim = 0) p_mask = np.zeros(target.shape) for i in maskindices: p_mask[i[0]][i[1]] = pred_labels[i[0]][i[1]].item() f, ax = plt.subplots(1,2,figsize=(20, 10)) ax[0].imshow(target) ax[1].imshow(p_mask) ax[0].set_title(&quot;ground truth&quot;) ax[1].set_title(&quot;prediction&quot;) plt.show() . Visualize predictions . I would have liked to include mapping of colors to labels here. . predict_mask(preds[1][2],preds[0][2],False) p = label_predict(preds[1][2],preds[0][2],codes_dict,False) g = vset.iloc[1][0] print(&quot;predicted label: &quot;, p) print(&quot;correct label: &quot;, g) . predicted label: Grassland shrubland correct label: Grassland shrubland . for i, j in enumerate(vset.head(10).values): f, ax = plt.subplots(1,2,figsize=(20, 10)) im = PILImage.create(&quot;../input/forestnet/ForestNetDataset/&quot;f&#39;{j[1]}&#39;&quot;/images/visible/composite.png&quot;) ax[0].imshow(im) ax[1].imshow(preds[0][i].argmax(dim = 0)) plt.show() . Examine model performance on test data . I use the composite images as input for the model on test data to ensure good quality of the input image. In the ForestNet paper the image captured temporally closest to the year of the forest loss event was primarily used as input. . test = pd.read_csv(&#39;../input/forestnet/ForestNetDataset/test.csv&#39;) test.drop(columns = [&#39;label&#39;,&#39;latitude&#39;,&#39;longitude&#39;,&#39;year&#39;], inplace = True) . def t_acc(df,codes): # undefined_count = 0 acc = [] label = df.iloc[0][0] for x in df.values: im = PILImage.create(&quot;../input/forestnet/ForestNetDataset/&quot;f&#39;{x[1]}&#39;&quot;/images/visible/composite.png&quot;) AOI = createPILMask(getForestLoss(x[1]),x[0],codes) p = learn.predict(im) acc.append(label_predict(AOI.crop_pad(p[2].shape[1]),p[2],codes,True)) # print(len([x for x in acc if &quot;Undefined&quot; in x])) return (len([x for x in acc if label in x]) / len(acc)) . t_plant = test[test.merged_label == &#39;Plantation&#39;] t_other = test[test.merged_label == &#39;Other&#39;] t_grass = test[test.merged_label == &#39;Grassland shrubland&#39;] t_small = test[test.merged_label == &#39;Smallholder agriculture&#39;] plantation = t_acc(t_plant,codes_dict) other = t_acc(t_other,codes_dict) grass = t_acc(t_grass,codes_dict) small = t_acc(t_small,codes_dict) . print(&#39;plantation accuracy: &#39;, np.around(plantation,4)) print(&#39;other accuracy: &#39;, np.around(other,4)) print(&#39;grassland shrubland accuracy: &#39;, np.around(grass,4)) print(&#39;smallholder agriculture accuracy: &#39;, np.around(small,4)) . plantation accuracy: 0.7509 other accuracy: 0.3113 grassland shrubland accuracy: 0.5455 smallholder agriculture accuracy: 0.6435 .",
            "url": "https://oscarsommervold.github.io/DAT255-project/2021/04/24/forestnet-segmentation.html",
            "relUrl": "/2021/04/24/forestnet-segmentation.html",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://oscarsommervold.github.io/DAT255-project/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://oscarsommervold.github.io/DAT255-project/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://oscarsommervold.github.io/DAT255-project/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://oscarsommervold.github.io/DAT255-project/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}